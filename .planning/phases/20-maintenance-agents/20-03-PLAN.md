---
phase: 20-maintenance-agents
plan: 03
type: execute
wave: 2
depends_on: ["20-02"]
files_modified:
  - orchestrator/src/lib/maintenance/archival.ts
  - orchestrator/src/lib/maintenance/merge.ts
  - orchestrator/src/lib/maintenance/index.ts
autonomous: true

must_haves:
  truths:
    - "Signals can be archived (soft delete via status change)"
    - "Archived signals preserve all data and links"
    - "Duplicate signals can be merged with link transfer"
    - "Merge preserves provenance by archiving secondary signal"
    - "Activity log records archival and merge operations"
  artifacts:
    - path: "orchestrator/src/lib/maintenance/archival.ts"
      provides: "archiveSignals and unarchiveSignals functions"
      exports: ["archiveSignals", "unarchiveSignals", "ArchivalResult", "ArchivalCriteria"]
    - path: "orchestrator/src/lib/maintenance/merge.ts"
      provides: "mergeSignals function"
      exports: ["mergeSignals", "MergeResult"]
  key_links:
    - from: "orchestrator/src/lib/maintenance/archival.ts"
      to: "orchestrator/src/lib/db/schema.ts"
      via: "activityLogs insert"
      pattern: "db\\.insert\\(activityLogs\\)"
    - from: "orchestrator/src/lib/maintenance/merge.ts"
      to: "orchestrator/src/lib/db/schema.ts"
      via: "signalProjects, signalPersonas transfer"
      pattern: "db\\.insert\\(signalProjects\\)"
---

<objective>
Create archival and merge workflow modules for signal hygiene operations

Purpose: Enable soft-delete archival (status change, not data deletion) and duplicate merging with full provenance preservation. Key principle: signals are evidence and must be preserved for audit.

Output: Archival workflow for bulk/time-based archiving and merge workflow for consolidating duplicates
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/20-maintenance-agents/20-RESEARCH.md

# Required prior plan
@.planning/phases/20-maintenance-agents/20-02-SUMMARY.md (detection modules)

# Pattern references
@orchestrator/src/lib/db/schema.ts (activityLogs, signalProjects, signalPersonas)
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create archival workflow module</name>
  <files>orchestrator/src/lib/maintenance/archival.ts</files>
  <action>
Create `archival.ts` with archive and unarchive functions:

```typescript
/**
 * Signal Archival Workflow
 *
 * Soft-delete archival for signals - status change to "archived"
 * while preserving all data and links.
 *
 * Key principle: Signals are evidence and must never be permanently deleted.
 * Archival hides them from active views but preserves for audit.
 */

import { db } from "@/lib/db";
import { signals, activityLogs } from "@/lib/db/schema";
import { eq, and, lt, inArray } from "drizzle-orm";
import { nanoid } from "nanoid";

export interface ArchivalResult {
  archivedCount: number;
  signalIds: string[];
  archivedAt: Date;
}

export interface ArchivalCriteria {
  // Archive signals in "linked" status older than N days
  linkedOlderThanDays?: number;
  // Archive signals in "reviewed" status older than N days
  reviewedOlderThanDays?: number;
  // Manual selection of specific signal IDs
  signalIds?: string[];
}

/**
 * Archive signals based on criteria.
 * Preserves all data - just updates status to "archived".
 *
 * @param workspaceId - Workspace containing signals
 * @param criteria - What signals to archive
 * @param userId - User performing the action (null for automation)
 */
export async function archiveSignals(
  workspaceId: string,
  criteria: ArchivalCriteria,
  userId?: string
): Promise<ArchivalResult> {
  const now = new Date();
  let signalIdsToArchive: string[] = [];

  // Manual selection
  if (criteria.signalIds && criteria.signalIds.length > 0) {
    signalIdsToArchive = criteria.signalIds;
  }

  // Time-based linked signals
  if (criteria.linkedOlderThanDays) {
    const threshold = new Date();
    threshold.setDate(threshold.getDate() - criteria.linkedOlderThanDays);

    const oldLinked = await db
      .select({ id: signals.id })
      .from(signals)
      .where(
        and(
          eq(signals.workspaceId, workspaceId),
          eq(signals.status, "linked"),
          lt(signals.updatedAt, threshold)
        )
      );

    signalIdsToArchive.push(...oldLinked.map((s) => s.id));
  }

  // Time-based reviewed signals
  if (criteria.reviewedOlderThanDays) {
    const threshold = new Date();
    threshold.setDate(threshold.getDate() - criteria.reviewedOlderThanDays);

    const oldReviewed = await db
      .select({ id: signals.id })
      .from(signals)
      .where(
        and(
          eq(signals.workspaceId, workspaceId),
          eq(signals.status, "reviewed"),
          lt(signals.updatedAt, threshold)
        )
      );

    signalIdsToArchive.push(...oldReviewed.map((s) => s.id));
  }

  // Deduplicate
  signalIdsToArchive = [...new Set(signalIdsToArchive)];

  if (signalIdsToArchive.length === 0) {
    return { archivedCount: 0, signalIds: [], archivedAt: now };
  }

  // Archive signals (update status, preserve everything else)
  await db
    .update(signals)
    .set({
      status: "archived",
      updatedAt: now,
    })
    .where(
      and(
        eq(signals.workspaceId, workspaceId),
        inArray(signals.id, signalIdsToArchive)
      )
    );

  // Log activity for audit trail
  await db.insert(activityLogs).values({
    id: nanoid(),
    workspaceId,
    userId: userId ?? null,
    action: "signals.archived",
    targetType: "signals",
    targetId: null,
    metadata: {
      count: signalIdsToArchive.length,
      signalIds: signalIdsToArchive,
      criteria,
      actor: userId ? "user" : "automation",
    },
    createdAt: now,
  });

  return {
    archivedCount: signalIdsToArchive.length,
    signalIds: signalIdsToArchive,
    archivedAt: now,
  };
}

/**
 * Restore archived signals to "reviewed" status.
 *
 * @param workspaceId - Workspace containing signals
 * @param signalIds - Signal IDs to restore
 * @param userId - User performing the action
 */
export async function unarchiveSignals(
  workspaceId: string,
  signalIds: string[],
  userId?: string
): Promise<{ restoredCount: number }> {
  const now = new Date();

  // Restore to "reviewed" status (not "new" since they were processed)
  const result = await db
    .update(signals)
    .set({
      status: "reviewed",
      updatedAt: now,
    })
    .where(
      and(
        eq(signals.workspaceId, workspaceId),
        eq(signals.status, "archived"),
        inArray(signals.id, signalIds)
      )
    );

  // Log activity
  await db.insert(activityLogs).values({
    id: nanoid(),
    workspaceId,
    userId: userId ?? null,
    action: "signals.unarchived",
    targetType: "signals",
    targetId: null,
    metadata: {
      count: signalIds.length,
      signalIds,
    },
    createdAt: now,
  });

  return { restoredCount: signalIds.length };
}

/**
 * Get count of archivable signals (linked/reviewed older than thresholds).
 */
export async function getArchivableCount(
  workspaceId: string,
  linkedOlderThanDays: number,
  reviewedOlderThanDays: number
): Promise<{ linked: number; reviewed: number }> {
  const linkedThreshold = new Date();
  linkedThreshold.setDate(linkedThreshold.getDate() - linkedOlderThanDays);

  const reviewedThreshold = new Date();
  reviewedThreshold.setDate(reviewedThreshold.getDate() - reviewedOlderThanDays);

  const [linkedResult, reviewedResult] = await Promise.all([
    db
      .select({ id: signals.id })
      .from(signals)
      .where(
        and(
          eq(signals.workspaceId, workspaceId),
          eq(signals.status, "linked"),
          lt(signals.updatedAt, linkedThreshold)
        )
      ),
    db
      .select({ id: signals.id })
      .from(signals)
      .where(
        and(
          eq(signals.workspaceId, workspaceId),
          eq(signals.status, "reviewed"),
          lt(signals.updatedAt, reviewedThreshold)
        )
      ),
  ]);

  return {
    linked: linkedResult.length,
    reviewed: reviewedResult.length,
  };
}
```
  </action>
  <verify>
Run: `cd /Users/tylersahagun/Source/elmer/orchestrator && npx tsc --noEmit`
Test: `grep -n "archiveSignals\|unarchiveSignals" src/lib/maintenance/archival.ts`
  </verify>
  <done>archival.ts exists with archiveSignals, unarchiveSignals, and getArchivableCount functions</done>
</task>

<task type="auto">
  <name>Task 2: Create merge workflow module</name>
  <files>orchestrator/src/lib/maintenance/merge.ts</files>
  <action>
Create `merge.ts` with duplicate merge functionality:

```typescript
/**
 * Signal Merge Workflow
 *
 * Merges duplicate signals by transferring links from secondary
 * to primary signal, then archiving the secondary.
 *
 * Strategy:
 * 1. Keep the older signal as primary (has more history)
 * 2. Transfer all project/persona links from secondary to primary
 * 3. Archive the secondary signal (preserve, don't delete)
 * 4. Log merge in activity log for audit
 */

import { db } from "@/lib/db";
import {
  signals,
  signalProjects,
  signalPersonas,
  activityLogs,
} from "@/lib/db/schema";
import { eq, and, inArray } from "drizzle-orm";
import { nanoid } from "nanoid";

export interface MergeResult {
  primarySignalId: string;
  mergedSignalId: string;
  projectsTransferred: number;
  personasTransferred: number;
}

/**
 * Merge two duplicate signals.
 *
 * @param workspaceId - Workspace containing signals
 * @param primarySignalId - Signal to keep (receives links)
 * @param secondarySignalId - Signal to merge into primary (gets archived)
 * @param userId - User performing the merge
 */
export async function mergeSignals(
  workspaceId: string,
  primarySignalId: string,
  secondarySignalId: string,
  userId?: string
): Promise<MergeResult> {
  const now = new Date();

  // Verify both signals exist and belong to workspace
  const [primary, secondary] = await Promise.all([
    db
      .select()
      .from(signals)
      .where(
        and(eq(signals.id, primarySignalId), eq(signals.workspaceId, workspaceId))
      )
      .limit(1),
    db
      .select()
      .from(signals)
      .where(
        and(
          eq(signals.id, secondarySignalId),
          eq(signals.workspaceId, workspaceId)
        )
      )
      .limit(1),
  ]);

  if (primary.length === 0 || secondary.length === 0) {
    throw new Error("One or both signals not found in workspace");
  }

  // Transfer project links from secondary to primary
  const existingProjectLinks = await db
    .select({ projectId: signalProjects.projectId })
    .from(signalProjects)
    .where(eq(signalProjects.signalId, primarySignalId));

  const existingProjectIds = new Set(existingProjectLinks.map((l) => l.projectId));

  const secondaryProjectLinks = await db
    .select()
    .from(signalProjects)
    .where(eq(signalProjects.signalId, secondarySignalId));

  let projectsTransferred = 0;
  for (const link of secondaryProjectLinks) {
    if (!existingProjectIds.has(link.projectId)) {
      await db.insert(signalProjects).values({
        id: nanoid(),
        signalId: primarySignalId,
        projectId: link.projectId,
        linkedAt: link.linkedAt,
        linkedBy: link.linkedBy,
        linkReason: `Merged from signal ${secondarySignalId}: ${link.linkReason || ""}`.trim(),
        confidence: link.confidence,
      });
      projectsTransferred++;
    }
  }

  // Transfer persona links from secondary to primary
  const existingPersonaLinks = await db
    .select({ personaId: signalPersonas.personaId })
    .from(signalPersonas)
    .where(eq(signalPersonas.signalId, primarySignalId));

  const existingPersonaIds = new Set(existingPersonaLinks.map((l) => l.personaId));

  const secondaryPersonaLinks = await db
    .select()
    .from(signalPersonas)
    .where(eq(signalPersonas.signalId, secondarySignalId));

  let personasTransferred = 0;
  for (const link of secondaryPersonaLinks) {
    if (!existingPersonaIds.has(link.personaId)) {
      await db.insert(signalPersonas).values({
        id: nanoid(),
        signalId: primarySignalId,
        personaId: link.personaId,
        linkedAt: link.linkedAt,
        linkedBy: link.linkedBy,
      });
      personasTransferred++;
    }
  }

  // Archive the secondary signal (not delete - preserve history)
  await db
    .update(signals)
    .set({
      status: "archived",
      updatedAt: now,
    })
    .where(eq(signals.id, secondarySignalId));

  // Log the merge activity for audit trail
  await db.insert(activityLogs).values({
    id: nanoid(),
    workspaceId,
    userId: userId ?? null,
    action: "signals.merged",
    targetType: "signals",
    targetId: primarySignalId,
    metadata: {
      primarySignalId,
      secondarySignalId,
      projectsTransferred,
      personasTransferred,
      primaryVerbatim: primary[0].verbatim.slice(0, 100),
      secondaryVerbatim: secondary[0].verbatim.slice(0, 100),
    },
    createdAt: now,
  });

  return {
    primarySignalId,
    mergedSignalId: secondarySignalId,
    projectsTransferred,
    personasTransferred,
  };
}

/**
 * Dismiss a duplicate pair (mark as not duplicates).
 * Records in activity log so pair won't be suggested again.
 */
export async function dismissDuplicatePair(
  workspaceId: string,
  signalId1: string,
  signalId2: string,
  userId?: string
): Promise<void> {
  const now = new Date();

  // Log dismissal so we can filter it out in future suggestions
  await db.insert(activityLogs).values({
    id: nanoid(),
    workspaceId,
    userId: userId ?? null,
    action: "duplicates.dismissed",
    targetType: "signals",
    targetId: null,
    metadata: {
      signalId1,
      signalId2,
      pairId: [signalId1, signalId2].sort().join("-"),
    },
    createdAt: now,
  });
}
```
  </action>
  <verify>
Run: `cd /Users/tylersahagun/Source/elmer/orchestrator && npx tsc --noEmit`
Test: `grep -n "mergeSignals\|dismissDuplicatePair" src/lib/maintenance/merge.ts`
  </verify>
  <done>merge.ts exists with mergeSignals and dismissDuplicatePair functions</done>
</task>

<task type="auto">
  <name>Task 3: Update maintenance index with workflow exports</name>
  <files>orchestrator/src/lib/maintenance/index.ts</files>
  <action>
Update `index.ts` to include archival and merge exports:

```typescript
/**
 * Maintenance Module
 *
 * Signal hygiene utilities for orphan detection, duplicate detection,
 * archival workflows, and cleanup suggestions.
 *
 * Usage: import { findOrphanSignals, archiveSignals } from "@/lib/maintenance";
 */

// Detection
export {
  findOrphanSignals,
  getOrphanCount,
  type OrphanSignal,
  type OrphanDetectionResult,
} from "./orphan-detector";

export {
  findDuplicateSignals,
  getDuplicateCount,
  type DuplicatePair,
  type DuplicateDetectionResult,
} from "./duplicate-detector";

// Workflows
export {
  archiveSignals,
  unarchiveSignals,
  getArchivableCount,
  type ArchivalResult,
  type ArchivalCriteria,
} from "./archival";

export {
  mergeSignals,
  dismissDuplicatePair,
  type MergeResult,
} from "./merge";
```
  </action>
  <verify>
Run: `cd /Users/tylersahagun/Source/elmer/orchestrator && npx tsc --noEmit`
Test: `grep -n "archiveSignals\|mergeSignals" src/lib/maintenance/index.ts`
  </verify>
  <done>index.ts exports all detection and workflow utilities</done>
</task>

</tasks>

<verification>
- [ ] TypeScript compiles without errors
- [ ] archival.ts exports archiveSignals, unarchiveSignals, getArchivableCount
- [ ] merge.ts exports mergeSignals, dismissDuplicatePair
- [ ] archiveSignals updates status to "archived" (soft delete)
- [ ] mergeSignals transfers links and archives secondary signal
- [ ] Both workflows log to activityLogs for audit
- [ ] index.ts re-exports all utilities
</verification>

<success_criteria>
1. archiveSignals accepts criteria (time-based or manual selection) and updates status
2. unarchiveSignals restores signals to "reviewed" status
3. mergeSignals transfers project/persona links and archives secondary
4. Activity logs record all operations for audit trail
5. All functions compile and type-check correctly
</success_criteria>

<output>
After completion, create `.planning/phases/20-maintenance-agents/20-03-SUMMARY.md`
</output>
